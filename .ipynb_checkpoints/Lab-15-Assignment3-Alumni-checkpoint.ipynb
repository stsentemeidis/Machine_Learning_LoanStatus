{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 16 Assignment 3 - Group Assignment - Group O-1-7\n",
    "\n",
    "When creating ML models, the concept of efficiency has three sides:\n",
    "1. The time dedicated by the analyst to build the model\n",
    "2. The computer time and resources needed by the final model\n",
    "3. The accuracy of the final model\n",
    "\n",
    "Efficiency is a combination of all\n",
    "\n",
    "In this assignment, you are asked to be efficient. Spark is the best tool to build models over massive datasets\n",
    "\n",
    "If you need to create Spark+Python Machine Learning models that \"run fast\" on the  cluster, you must avoid using Python code or working with RRD+python. Try to use  the already existing methods that do what you need (do not reinvent the wheel).\n",
    "\n",
    "Therefore try to use the implemented object+methods inside the Spark SQL and ML modules. They are very fast, because it is compiled Java/Scala code. Try to use: DataFrames, Feature Transfomers, Estimators, Pipelines, GridSearch, CV, ...\n",
    "\n",
    "For this assignment, you are asked to create a classification model that:\n",
    "1. Uses the variables in the dataset (train.csv) to predict label \"loan_status\"\n",
    "2. Write a python scripts that:\n",
    "    - Reads the \"train.csv\" and \"test.csv\" files, transform and select variables as you wish.\n",
    "    - Train/fit your model using the \"train.csv\".\n",
    "    - Predict your model on the \"test.csv\" ( you should generate a file with your predictions).\n",
    "    - I will use a different test dataset (with the true loan_status).\n",
    "\n",
    "Your work will be evaluated under the following scoring schema\n",
    "- (40%) ETL process\n",
    "- (40%) Model train process\n",
    "- (10%) Code Readability \n",
    "- (10%) AUC on the test set (at least 50%)\n",
    "\n",
    "Enjoy it and best of luck!!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Assignment is based on kaggle competition https://www.kaggle.com/c/loan-default-prediction from where a sub-dataset has been taken.\n",
    "\n",
    "### File Description\n",
    "**train.csv** - the training set (to use for building a model)\n",
    "\n",
    "**test.csv** - the test set (to use for applying predictings)\n",
    "\n",
    "**sample_submission.csv** - a template for the submission file\n",
    "\n",
    "### Data Description (contained in LendingClub_DataDescription.csv)\n",
    "**ID**: A unique LC assigned ID for the loan listing.\n",
    "\n",
    "**loan_amnt**: The listed amount of the loan applied for by the borrower. If at some point in time, the credit department reduces the loan amount, then it will be reflected in this value.\n",
    "\n",
    "**loan_status**: Current status of the loan (**Target**: 1 = Charged Off, 0 = Fully Paid).\n",
    "\n",
    "**term**: The number of payments on the loan. Values are in months and can be either 36 or 60.\n",
    "\n",
    "**int_rate**: Interest Rate on the loan.\n",
    "\n",
    "**installment**: The monthly payment owed by the borrower if the loan originates.\n",
    "\n",
    "**emp_length**: Employment length in years. Possible values are between 0 and 10 where 0 means less than one year and 10 means ten or more years.\n",
    "\n",
    "**home_ownership**: The home ownership status provided by the borrower during registration. Our values are: OTHER/NONE, MORTGAGE, OWN, RENT.\n",
    "\n",
    "**annual_inc**: The self-reported annual income provided by the borrower during registration.\n",
    "\n",
    "**purpose**: A category provided by the borrower for the loan request.\n",
    "\n",
    "**title**: The loan title provided by the borrower.\n",
    "\n",
    "**STATE**: The state provided by the borrower in the loan application.\n",
    "\n",
    "**delinq_2yrs**: The number of 30+ days past-due incidences of delinquency in the borrower's credit file for the past 2 years.\n",
    "\n",
    "**revol_bal**: Total credit revolving balance.\n",
    "\n",
    "**revol_util**: Revolving line utilization rate, or the amount of credit the borrower is using relative to all available revolving credit.\n",
    "\n",
    "**total_pymnt**: Indicates total payment at the end of the loan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ['SPARK_HOME'] = \"/Users/stavrostsentemeidis/Desktop/Install_Spark/spark-2.3.2-bin-hadoop2.7/\"\n",
    "\n",
    "# Create a variable for our root path\n",
    "SPARK_HOME = os.environ['SPARK_HOME']\n",
    "\n",
    "#Add the following paths to the system path. Please check your installation\n",
    "#to make sure that these zip files actually exist. The names might change\n",
    "#as versions change.\n",
    "sys.path.insert(0,os.path.join(SPARK_HOME,\"python\"))\n",
    "sys.path.insert(0,os.path.join(SPARK_HOME,\"python\",\"lib\"))\n",
    "sys.path.insert(0,os.path.join(SPARK_HOME,\"python\",\"lib\",\"pyspark.zip\"))\n",
    "sys.path.insert(0,os.path.join(SPARK_HOME,\"python\",\"lib\",\"py4j-0.10.7-src.zip\"))\n",
    "\n",
    "#Initialize SparkSession and SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import isnan, when, count\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.classification import RandomForestClassificationModel, RandomForestClassifier\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator, CrossValidatorModel\n",
    "from pyspark.ml.param import Params\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoderEstimator\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "#Create a Spark Session\n",
    "MySparkSession = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .appName(\"MiPrimer\") \\\n",
    "    .config(\"spark.executor.memory\", \"6g\") \\\n",
    "    .config(\"spark.cores.max\",\"4\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "#Get the Spark Context from Spark Session    \n",
    "MySparkContext = MySparkSession.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading & Displaying Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'Path does not exist: file:/Users/stavrostsentemeidis/Documents/GitHub/data/train.csv;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/Desktop/Install_Spark/spark-2.3.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Install_Spark/spark-2.3.2-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o123.load.\n: org.apache.spark.sql.AnalysisException: Path does not exist: file:/Users/stavrostsentemeidis/Documents/GitHub/data/train.csv;\n\tat org.apache.spark.sql.execution.datasources.DataSource$.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:719)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$15.apply(DataSource.scala:390)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$15.apply(DataSource.scala:390)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.foreach(List.scala:381)\n\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.flatMap(List.scala:344)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:389)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:239)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:227)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:174)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-b65a7b859327>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloanDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMySparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'csv'\u001b[0m\u001b[0;34m)\u001b[0m                 \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"inferSchema\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"true\"\u001b[0m\u001b[0;34m)\u001b[0m                 \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"delimiter\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\";\"\u001b[0m\u001b[0;34m)\u001b[0m                 \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'header'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'true'\u001b[0m\u001b[0;34m)\u001b[0m                 \u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtestDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMySparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'csv'\u001b[0m\u001b[0;34m)\u001b[0m                 \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"inferSchema\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"true\"\u001b[0m\u001b[0;34m)\u001b[0m                 \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"delimiter\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\";\"\u001b[0m\u001b[0;34m)\u001b[0m                 \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'header'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'true'\u001b[0m\u001b[0;34m)\u001b[0m                 \u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Install_Spark/spark-2.3.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Install_Spark/spark-2.3.2-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Install_Spark/spark-2.3.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'Path does not exist: file:/Users/stavrostsentemeidis/Documents/GitHub/data/train.csv;'"
     ]
    }
   ],
   "source": [
    "loanDF = MySparkSession.read.format('csv') \\\n",
    "                .option(\"inferSchema\", \"true\") \\\n",
    "                .option(\"delimiter\", \";\") \\\n",
    "                .option('header','true') \\\n",
    "                .load('../data/train.csv') \n",
    "\n",
    "testDF = MySparkSession.read.format('csv') \\\n",
    "                .option(\"inferSchema\", \"true\") \\\n",
    "                .option(\"delimiter\", \";\") \\\n",
    "                .option('header','true') \\\n",
    "                .load('../data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loanDF.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "testDF.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA | Null Values | Cross Table Distribution | Covariances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary of Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loanDF.printSchema()\n",
    "testDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Renaming | Describing | Changing Data Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loanDF = loanDF.withColumn('int_rate', regexp_replace('int_rate', '%', ''))\n",
    "loanDF = loanDF.withColumn('title', regexp_replace('int_rate', '.', ''))\n",
    "loanDF = loanDF.withColumn(\"int_rate\", loanDF[\"int_rate\"].cast(\"decimal(10,0)\"))\n",
    "loanDF = loanDF.withColumn(\"revol_util\", loanDF[\"revol_util\"].cast(\"decimal(10,0)\"))\n",
    "\n",
    "testDF = testDF.withColumn('int_rate', regexp_replace('int_rate', '%', ''))\n",
    "testDF = testDF.withColumn('int_rate', regexp_replace('int_rate', '.', ''))\n",
    "testDF = testDF.withColumn(\"int_rate\", testDF[\"int_rate\"].cast(\"decimal(10,0)\"))\n",
    "testDF = testDF.withColumn(\"revol_util\", testDF[\"revol_util\"].cast(\"decimal(10,0)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loanDF = loanDF.withColumnRenamed(\"ID\",\"id\")\n",
    "loanDF = loanDF.withColumnRenamed(\"loan_amnt\",\"loan_amount\")\n",
    "loanDF = loanDF.withColumnRenamed(\"term\",\"term\")\n",
    "loanDF = loanDF.withColumnRenamed(\"home_ownership\",\"home_ownership\")\n",
    "loanDF = loanDF.withColumnRenamed(\"int_rate\",\"interest_rate\")\n",
    "loanDF = loanDF.withColumnRenamed(\"installment\",\"monthly_payment\")\n",
    "loanDF = loanDF.withColumnRenamed(\"emp_length\",\"employment_time\")\n",
    "loanDF = loanDF.withColumnRenamed(\"delinq_2yrs\",\"deliquency_past_2years\")\n",
    "loanDF = loanDF.withColumnRenamed(\"revol_bal\",\"revolving_balance\")\n",
    "loanDF = loanDF.withColumnRenamed(\"revol_util\",\"revolving_utilization_rate\")\n",
    "loanDF = loanDF.withColumnRenamed(\"total_pymnt\",\"total_payment\")\n",
    "loanDF = loanDF.withColumnRenamed(\"purpose\",\"loan_purpose\")\n",
    "loanDF = loanDF.withColumnRenamed(\"annual_inc\",\"annual_income\")\n",
    "loanDF = loanDF.withColumnRenamed(\"STATE\",\"state\")\n",
    "loanDF = loanDF.withColumnRenamed(\"installment\",\"installment\")\n",
    "loanDF = loanDF.withColumnRenamed(\"loan_status\",\"loan_status\")\n",
    "\n",
    "\n",
    "testDF = testDF.withColumnRenamed(\"ID\",\"id\")\n",
    "testDF = testDF.withColumnRenamed(\"loan_amnt\",\"loan_amount\")\n",
    "testDF = testDF.withColumnRenamed(\"term\",\"term\")\n",
    "testDF = testDF.withColumnRenamed(\"home_ownership\",\"home_ownership\")\n",
    "testDF = testDF.withColumnRenamed(\"int_rate\",\"interest_rate\")\n",
    "testDF = testDF.withColumnRenamed(\"installment\",\"monthly_payment\")\n",
    "testDF = testDF.withColumnRenamed(\"emp_length\",\"employment_time\")\n",
    "testDF = testDF.withColumnRenamed(\"delinq_2yrs\",\"deliquency_past_2years\")\n",
    "testDF = testDF.withColumnRenamed(\"revol_bal\",\"revolving_balance\")\n",
    "testDF = testDF.withColumnRenamed(\"revol_util\",\"revolving_utilization_rate\")\n",
    "testDF = testDF.withColumnRenamed(\"total_pymnt\",\"total_payment\")\n",
    "testDF = testDF.withColumnRenamed(\"purpose\",\"loan_purpose\")\n",
    "testDF = testDF.withColumnRenamed(\"annual_inc\",\"annual_income\")\n",
    "testDF = testDF.withColumnRenamed(\"STATE\",\"state\")\n",
    "testDF = testDF.withColumnRenamed(\"installment\",\"installment\")\n",
    "testDF = testDF.withColumnRenamed(\"loan_status\",\"loan_status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "numeric_df = ['loan_amount','interest_rate', 'monthly_payment','annual_income', 'deliquency_past_2years',\n",
    "              'total_payment','revolving_balance','revolving_utilization_rate']\n",
    "\n",
    "categorical_Df = ['term','employment_time', 'home_ownership', 'loan_purpose', 'title','state']\n",
    "\n",
    "loanDF.describe('loan_amount','term','interest_rate','title','employment_time','home_ownership').show()\n",
    "loanDF.describe('annual_income','loan_purpose','monthly_payment','state','deliquency_past_2years').show()\n",
    "loanDF.describe('revolving_balance','revolving_utilization_rate','total_payment','loan_status').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(testDF.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loanDF.count())\n",
    "print(testDF.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking Null Values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_loanDF = loanDF.na.drop() \n",
    "remove_testDF = testDF.na.drop()  \n",
    "print(remove_loanDF.count())\n",
    "print(remove_testDF.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Table Distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross tables distribution\n",
    "loanDF.stat.crosstab('term','employment_time').show()\n",
    "loanDF.stat.crosstab('term','home_ownership').show()\n",
    "loanDF.stat.crosstab('term','loan_purpose').show()\n",
    "loanDF.stat.crosstab('term','state').show()\n",
    "loanDF.stat.crosstab('employment_time','home_ownership').show()\n",
    "loanDF.stat.crosstab('employment_time','loan_purpose').show()\n",
    "loanDF.stat.crosstab('employment_time','state').show()\n",
    "loanDF.stat.crosstab('home_ownership','loan_purpose').show()\n",
    "loanDF.stat.crosstab('home_ownership','state').show()\n",
    "loanDF.stat.crosstab('loan_purpose','state').show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Covariance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# covariance\n",
    "for i in numeric_df:\n",
    "    for j in numeric_df:\n",
    "        print('Covariance of ' + i + ' and '+ j )\n",
    "        print(loanDF.stat.cov(i, j))\n",
    "        print(\"\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ETL summary |  Spark code for Imputing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we present the steps we decided to follow during our EDA in order to prepare our dataset for our machine learning implementation. It is worth mentioning that some of the steps had to be included in the previous part of the assignment in order to have a complete overview of the *cross table distributions* and the *covariances*.\n",
    "\n",
    "1. **interest_rate** : remove special % character and change datatype from *string* to *decimal*.\n",
    "2. **revol_util** : change datatype from *string* to *decimal*.\n",
    "3. **Rename** columns to be better represented.\n",
    "4. **Trim** the variable title as there are multiple unnecessary dots.\n",
    "5. **Checkin for Na**: \n",
    "    1. Number of rows with NA for *loanDF*:    29755\n",
    "    2. Number of rows without NA for *loanDF*: 27773\n",
    "    3. Number of rows with NA for *testDF*:    10024\n",
    "    4. Number of rows without NA for *testDF*: 9116\n",
    "6. **Filling Na values**:\n",
    "    1. **title** :                      unknown\n",
    "    2. **loan_purpose** :               unknown\n",
    "    3. **state** :                      unknown\n",
    "    4. **deliquency_past_2years** :     -1\n",
    "    5. **revolving_balance** :          13350.529071398512 (avg)\n",
    "    6. **revolving_utilization_rate** : 0.5054 (avg)\n",
    "    7. **total_payment** :              12143.791490200982 (avg)\n",
    "7. **Drop** variables *title* and *state* cause they have too many unique values that cannot be grouped in order to apply one hot coding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Imputing Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loanDF = loanDF.na.fill({'title': 'uknown', 'loan_purpose': 'unknown', 'state': 'unknown',\n",
    "                         'deliquency_past_2years':-1,'revolving_balance':13350.529071398512,\n",
    "                         'revolving_utilization_rate':0.5054 , 'total_payment': 12143.791490200982})\n",
    "\n",
    "testDF = testDF.na.fill({'title': 'unknown', 'loan_purpose': 'uknown', 'state': 'unknown',\n",
    "                         'deliquency_past_2years':-1,'revolving_balance':13350.529071398512,\n",
    "                         'revolving_utilization_rate':0.5054 , 'total_payment': 12143.791490200982})\n",
    "print(loanDF.count())\n",
    "print(testDF.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Double checking final clean dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking all our columns to verify that we have:\n",
    "   * Distinct user id so we do not need to group by.\n",
    "   * Clean data format.\n",
    "   * Clean cell values, (for example no Na and nan, which mean the same thing) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loanDF.describe('loan_amount','term','interest_rate','title','employment_time','home_ownership').show()\n",
    "loanDF.describe('annual_income','loan_purpose','monthly_payment','state','deliquency_past_2years').show()\n",
    "loanDF.describe('revolving_balance','revolving_utilization_rate','total_payment','loan_status').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# we have unique id = number of rows of dataset, so we do not need to group by.\n",
    "for i in loanDF.columns:\n",
    "    print(loanDF.select(i).distinct().count())\n",
    "    print(loanDF.select(i).distinct().show())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage we check the final structure of the table and we also decide to drop the **title** and **state** feature as they have so many different values, which makes it difficult to use in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loanDF = loanDF.drop(\"title\", 'state')\n",
    "testDF = testDF.drop(\"title\", 'state')\n",
    "loanDF.printSchema()\n",
    "testDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stages = []\n",
    "\n",
    "# term_index = StringIndexer() \\\n",
    "#                     .setInputCol(\"term\") \\\n",
    "#                     .setOutputCol(\"term_index\") \\\n",
    "#                     .fit(loanDF)\n",
    "# stages.append(term_index)\n",
    "\n",
    "# employment_time_index = StringIndexer() \\\n",
    "#                 .setInputCol(\"employment_time\") \\\n",
    "#                 .setOutputCol(\"employment_time_index\") \\\n",
    "#                 .fit(loanDF)\n",
    "# stages.append(employment_time_index)\n",
    "\n",
    "# home_ownership_index = StringIndexer() \\\n",
    "#                     .setInputCol(\"home_ownership\") \\\n",
    "#                     .setOutputCol(\"home_ownership_index\") \\\n",
    "#                     .fit(loanDF)\n",
    "# stages.append(home_ownership_index)\n",
    "\n",
    "# loan_purpose_index = StringIndexer() \\\n",
    "#                 .setInputCol(\"loan_purpose\") \\\n",
    "#                 .setOutputCol(\"loan_purpose_index\") \\\n",
    "#                 .fit(loanDF)\n",
    "# stages.append(loan_purpose_index)\n",
    "\n",
    "# # state_index = StringIndexer() \\\n",
    "# #                     .setInputCol(\"state\") \\\n",
    "# #                     .setOutputCol(\"state_index\") \\\n",
    "# #                     .fit(loanDF)\n",
    "# # stages.append(state_index)\n",
    "\n",
    "# label_index = StringIndexer() \\\n",
    "#                 .setInputCol(\"loan_status\") \\\n",
    "#                 .setOutputCol(\"status\") \\\n",
    "#                 .fit(loanDF)\n",
    "# stages.append(label_index)\n",
    "\n",
    "# assembler = VectorAssembler(inputCols=[\"loan_amount\", 'term_index', 'interest_rate','monthly_payment',\n",
    "#                                       'employment_time_index', 'home_ownership_index', 'annual_income',\n",
    "#                                       'loan_purpose_index', 'deliquency_past_2years',\n",
    "#                                       'revolving_balance', 'revolving_utilization_rate','total_payment'],\n",
    "#                                         outputCol='features')\n",
    "\n",
    "# stages.append(assembler)\n",
    "\n",
    "# rf = RandomForestClassifier() \\\n",
    "#         .setFeaturesCol(\"features\") \\\n",
    "#         .setLabelCol(\"status\") \\\n",
    "#         .setSeed(100)\n",
    "# stages.append(rf)\n",
    "\n",
    "\n",
    "# pipeline = Pipeline(stages=stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluator = BinaryClassificationEvaluator() \\\n",
    "#                 .setLabelCol(\"loan_status\") \\\n",
    "#                 .setRawPredictionCol(\"rawPrediction\")\n",
    "\n",
    "# print(\"We are using metric: \" + evaluator.getMetricName())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Create Pipeline for train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainingData, validationData) = loanDF.randomSplit([0.7, 0.3], seed=100)\n",
    "print(trainingData.count())\n",
    "print(validationData.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regresion Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write a function \"metrics\" which has a LogisticRegressionModel.summary as input attribute and produces an output of: \n",
    "1. Area under ROC\n",
    "2. False Positive Rate By Label\n",
    "3. True Positive Rate By Label\n",
    "4. Precision By Label\n",
    "5. Recall By Label\n",
    "6. fMeasure By Label\n",
    "7. Accuracy\n",
    "8. False Positive Rate\n",
    "9. True Positive Rate\n",
    "10. fMeasure\n",
    "11. Precision\n",
    "12. Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def metrics(trainingSummary):  \n",
    "#     print(\"AUC: \" + str(evaluator.evaluate(trainingSummary)))\n",
    "#     print(\"False Positive Rate by Label: \" + x)\n",
    "#     print(\"True Positive Rate by Label: \" + x)\n",
    "#     print(\"Precision by Label: \" + x)\n",
    "#     print(\"Recall by Label: \" + x)\n",
    "#     print(\"fMeasure by Label: \" + x)\n",
    "#     print(\"Accuracy: \" + accuracy)\n",
    "#     print(\"False Positive Rate: \" + x)\n",
    "#     print(\"True Positive Rate: \" + x) \n",
    "#     print(\"fMeasure: \" + x)\n",
    "#     print(\"Precision: \" + x)\n",
    "#     print(\"Recall: \" + x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply a Logistic Regresion Base Model and show the metrics by the function above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we define the pipeline for the **Logistic Regression** model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stages_lr = []\n",
    "\n",
    "term_index = StringIndexer() \\\n",
    "                    .setInputCol(\"term\") \\\n",
    "                    .setOutputCol(\"term_index\") \\\n",
    "                    .fit(loanDF)\n",
    "stages_lr.append(term_index)\n",
    "\n",
    "employment_time_index = StringIndexer() \\\n",
    "                .setInputCol(\"employment_time\") \\\n",
    "                .setOutputCol(\"employment_time_index\") \\\n",
    "                .fit(loanDF)\n",
    "stages_lr.append(employment_time_index)\n",
    "\n",
    "home_ownership_index = StringIndexer() \\\n",
    "                    .setInputCol(\"home_ownership\") \\\n",
    "                    .setOutputCol(\"home_ownership_index\") \\\n",
    "                    .fit(loanDF)\n",
    "stages_lr.append(home_ownership_index)\n",
    "\n",
    "loan_purpose_index = StringIndexer() \\\n",
    "                .setInputCol(\"loan_purpose\") \\\n",
    "                .setOutputCol(\"loan_purpose_index\") \\\n",
    "                .fit(loanDF)\n",
    "stages_lr.append(loan_purpose_index)\n",
    "\n",
    "# state_index = StringIndexer() \\\n",
    "#                     .setInputCol(\"state\") \\\n",
    "#                     .setOutputCol(\"state_index\") \\\n",
    "#                     .fit(loanDF)\n",
    "# stages.append(state_index)\n",
    "\n",
    "label_index = StringIndexer() \\\n",
    "                .setInputCol(\"loan_status\") \\\n",
    "                .setOutputCol(\"status\") \\\n",
    "                .fit(loanDF)\n",
    "stages_lr.append(label_index)\n",
    "\n",
    "assembler_lr = VectorAssembler(inputCols=[\"loan_amount\", 'term_index', 'interest_rate','monthly_payment',\n",
    "                                      'employment_time_index', 'home_ownership_index', 'annual_income',\n",
    "                                      'loan_purpose_index', 'deliquency_past_2years',\n",
    "                                      'revolving_balance', 'revolving_utilization_rate'],\n",
    "                                        outputCol='features_lr')\n",
    "\n",
    "stages_lr.append(assembler_lr)\n",
    "\n",
    "lr = LogisticRegression(featuresCol = \"features_lr\", labelCol = \"status\")\n",
    "\n",
    "stages_lr.append(lr)\n",
    "\n",
    "\n",
    "pipeline_lr = Pipeline(stages=stages_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = BinaryClassificationEvaluator() \\\n",
    "                .setLabelCol(\"loan_status\") \\\n",
    "                .setRawPredictionCol(\"rawPrediction\")\n",
    "\n",
    "print(\"We are using metric: \" + evaluator.getMetricName())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_lr = pipeline_lr.fit(validationData)\n",
    "pipeline_results_lr = model_lr.transform(validationData)\n",
    "print(\"AUC: \" + str(evaluator.evaluate(pipeline_results_lr)))\n",
    "print();print('Model Parameters')\n",
    "print('----------------')\n",
    "print(lr.extractParamMap())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We are going to try to improve our model:\n",
    "1. Using a `weight column` in our Logistic Regression Model (Take into account we are working with a unbalanced dataset)\n",
    "2. Define a `ParamGridBuilder` with `regParam`, `elasticNetParam` and `maxIter` at least\n",
    "3. Define an `BinaryClassificationEvaluator`\n",
    "4. Using Cross Validation with a 5-fold `CrossValidator`\n",
    "\n",
    "Questions to answer:\n",
    "1. Have we improved the ROC-AUC?\n",
    "2. Which are the average ROC-AUC measurements in the different cross validation runs?\n",
    "3. Which are the parameters of the best model in the 5 k-fold runs?\n",
    "4. Which are the metrics of the best model (training) in the 5 k-fold runs? (Use the function above)\n",
    "5. Which is the ROC-AUC on validation dataset?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid_lr = ParamGridBuilder() \\\n",
    "                .addGrid(lr.regParam, [0.1, 0.5, 2.0]) \\\n",
    "                .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\\\n",
    "                .addGrid(lr.maxIter, [1, 5, 10])\\\n",
    "                .build()\n",
    "\n",
    "print(\"Param Grid: \" + str(paramGrid_lr))\n",
    "\n",
    "        \n",
    "cv_lr = CrossValidator() \\\n",
    "        .setEstimator(pipeline_lr) \\\n",
    "        .setEvaluator(evaluator) \\\n",
    "        .setEstimatorParamMaps(paramGrid_lr) \\\n",
    "        .setNumFolds(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_lr_model = cv_lr.fit(trainingData)\n",
    "cv_lr_results = cv_lr_model.transform(validationData)\n",
    "\n",
    "# Before we had 0.9127 and now\n",
    "print(\"AUC: \" + str(evaluator.evaluate(cv_lr_results)))\n",
    "\n",
    "# Means of model accuracy\n",
    "print(\"Means of metrics: \" + str(cv_lr_model.avgMetrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Random Forest Model\n",
    "1. Define a `ParamGridBuilder` with `maxDepth`, `numTrees` and `maxIter` at least\n",
    "2. Define an `BinaryClassificationEvaluator` (You can use the above one)\n",
    "3. Using Cross Validation with a 5-fold `CrossValidator`\n",
    "\n",
    "Questions to answer:\n",
    "\n",
    "1. Have we improved the ROC-AUC?\n",
    "2. Which are the average ROC-AUC measurements in the different cross validation runs?\n",
    "3. Which are the parameters of the best model in the 5 k-fold runs?\n",
    "4. Which is the importance of the features?\n",
    "5. Print full description of model.\n",
    "6. Which is the ROC-AUC on validation dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stages = []\n",
    "\n",
    "term_index = StringIndexer() \\\n",
    "                    .setInputCol(\"term\") \\\n",
    "                    .setOutputCol(\"term_index\") \\\n",
    "                    .fit(loanDF)\n",
    "stages.append(term_index)\n",
    "\n",
    "employment_time_index = StringIndexer() \\\n",
    "                .setInputCol(\"employment_time\") \\\n",
    "                .setOutputCol(\"employment_time_index\") \\\n",
    "                .fit(loanDF)\n",
    "stages.append(employment_time_index)\n",
    "\n",
    "home_ownership_index = StringIndexer() \\\n",
    "                    .setInputCol(\"home_ownership\") \\\n",
    "                    .setOutputCol(\"home_ownership_index\") \\\n",
    "                    .fit(loanDF)\n",
    "stages.append(home_ownership_index)\n",
    "\n",
    "loan_purpose_index = StringIndexer() \\\n",
    "                .setInputCol(\"loan_purpose\") \\\n",
    "                .setOutputCol(\"loan_purpose_index\") \\\n",
    "                .fit(loanDF)\n",
    "stages.append(loan_purpose_index)\n",
    "\n",
    "# state_index = StringIndexer() \\\n",
    "#                     .setInputCol(\"state\") \\\n",
    "#                     .setOutputCol(\"state_index\") \\\n",
    "#                     .fit(loanDF)\n",
    "# stages.append(state_index)\n",
    "\n",
    "label_index = StringIndexer() \\\n",
    "                .setInputCol(\"loan_status\") \\\n",
    "                .setOutputCol(\"status\") \\\n",
    "                .fit(loanDF)\n",
    "stages.append(label_index)\n",
    "\n",
    "assembler = VectorAssembler(inputCols=[\"loan_amount\", 'term_index', 'interest_rate','monthly_payment',\n",
    "                                      'employment_time_index', 'home_ownership_index', 'annual_income',\n",
    "                                      'loan_purpose_index', 'deliquency_past_2years',\n",
    "                                      'revolving_balance', 'revolving_utilization_rate'],\n",
    "                                        outputCol='features')\n",
    "\n",
    "stages.append(assembler)\n",
    "\n",
    "rf = RandomForestClassifier() \\\n",
    "        .setFeaturesCol(\"features\") \\\n",
    "        .setLabelCol(\"status\") \\\n",
    "        .setSeed(100)\n",
    "stages.append(rf)\n",
    "\n",
    "\n",
    "pipeline = Pipeline(stages=stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = BinaryClassificationEvaluator() \\\n",
    "                .setLabelCol(\"loan_status\") \\\n",
    "                .setRawPredictionCol(\"rawPrediction\")\n",
    "\n",
    "print(\"We are using metric: \" + evaluator.getMetricName())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "pipeline_results = model.transform(validationData)\n",
    "print(\"AUC: \" + str(evaluator.evaluate(pipeline_results)))\n",
    "\n",
    "print();print('Model Parameters')\n",
    "print('----------------')\n",
    "print(rf.extractParamMap())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By setting the **paramGrid_rf** variable we define the set of different parameters we would like to test in order to tune our algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid_rf = ParamGridBuilder() \\\n",
    "                .addGrid(rf.maxDepth, [10,20]) \\\n",
    "                .addGrid(rf.numTrees, [10,20]) \\\n",
    "                .build()\n",
    "\n",
    "print(\"Param Grid: \" + str(paramGrid_rf))\n",
    "\n",
    "cv_rf = CrossValidator() \\\n",
    "        .setEstimator(pipeline) \\\n",
    "        .setEvaluator(evaluator) \\\n",
    "        .setEstimatorParamMaps(paramGrid_rf) \\\n",
    "        .setNumFolds(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we apply a **Cross Validation** strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv_rf_model = cv_rf.fit(trainingData)\n",
    "# cv_rf_results = cv_rf_model.transform(validationData)\n",
    "\n",
    "# # Before we had 0.675 and now\n",
    "# print(\"AUC: \" + str(evaluator.evaluate(cv_rf_results)))\n",
    "\n",
    "# # Means of model accuracy\n",
    "# print(\"Means of metrics: \" + str(cv_rf_model.avgMetrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we still get lower results so we stick with our initial model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Gradient Boosting Model\n",
    "1. Defining a `ParamGridBuilder` with `maxDepth`, `numTrees` and `maxIter` at least (You can use the above one)\n",
    "2. Define an `BinaryClassificationEvaluator` (You can use the above one)\n",
    "3. Using Cross Validation with a 5-fold `CrossValidator`\n",
    "\n",
    "Questions to answer:\n",
    "\n",
    "1. Have we improved the ROC-AUC?\n",
    "2. Which are the average ROC-AUC measurements in the different cross validation runs?\n",
    "3. Which are the parameters of the best model in the 5 k-fold runs?\n",
    "4. Which is the importance of the features?\n",
    "5. Print full description of model.\n",
    "6. Which is the ROC-AUC on validation dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stages_gbm = []\n",
    "\n",
    "term_index = StringIndexer() \\\n",
    "                    .setInputCol(\"term\") \\\n",
    "                    .setOutputCol(\"term_index\") \\\n",
    "                    .fit(loanDF)\n",
    "stages_gbm.append(term_index)\n",
    "\n",
    "employment_time_index = StringIndexer() \\\n",
    "                .setInputCol(\"employment_time\") \\\n",
    "                .setOutputCol(\"employment_time_index\") \\\n",
    "                .fit(loanDF)\n",
    "stages_gbm.append(employment_time_index)\n",
    "\n",
    "home_ownership_index = StringIndexer() \\\n",
    "                    .setInputCol(\"home_ownership\") \\\n",
    "                    .setOutputCol(\"home_ownership_index\") \\\n",
    "                    .fit(loanDF)\n",
    "stages_gbm.append(home_ownership_index)\n",
    "\n",
    "loan_purpose_index = StringIndexer() \\\n",
    "                .setInputCol(\"loan_purpose\") \\\n",
    "                .setOutputCol(\"loan_purpose_index\") \\\n",
    "                .fit(loanDF)\n",
    "stages_gbm.append(loan_purpose_index)\n",
    "\n",
    "# state_index = StringIndexer() \\\n",
    "#                     .setInputCol(\"state\") \\\n",
    "#                     .setOutputCol(\"state_index\") \\\n",
    "#                     .fit(loanDF)\n",
    "# stages.append(state_index)\n",
    "\n",
    "label_index = StringIndexer() \\\n",
    "                .setInputCol(\"loan_status\") \\\n",
    "                .setOutputCol(\"status\") \\\n",
    "                .fit(loanDF)\n",
    "stages_gbm.append(label_index)\n",
    "\n",
    "assembler_gbm = VectorAssembler(inputCols=[\"loan_amount\", 'term_index', 'interest_rate','monthly_payment',\n",
    "                                      'employment_time_index', 'home_ownership_index', 'annual_income',\n",
    "                                      'loan_purpose_index', 'deliquency_past_2years',\n",
    "                                      'revolving_balance', 'revolving_utilization_rate'],\n",
    "                                        outputCol='features_gbm')\n",
    "\n",
    "stages_gbm.append(assembler_gbm)\n",
    "\n",
    "gbm = GBTClassifier(labelCol=\"status\", featuresCol=\"features_gbm\", maxIter=10)\n",
    "\n",
    "stages_gbm.append(gbm)\n",
    "\n",
    "\n",
    "pipeline_gbm = Pipeline(stages=stages_gbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_gbm = pipeline_gbm.fit(validationData)\n",
    "pipeline_results_gbm = model_gbm.transform(validationData)\n",
    "print(\"AUC: \" + str(evaluator.evaluate(pipeline_results_gbm)))\n",
    "print();print('Model Parameters')\n",
    "print('----------------')\n",
    "print(lr.extractParamMap())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid_gbm = ParamGridBuilder() \\\n",
    "                    .addGrid(GBTClassifier.maxDepth, [2,5])\\\n",
    "                    .addGrid(GBTClassifier.maxIter, [10,20])\\\n",
    "                    .build()\n",
    "\n",
    "print(\"Param Grid: \" + str(paramGrid_gbm))\n",
    "\n",
    "cv_gbm = CrossValidator() \\\n",
    "        .setEstimator(pipeline_gbm) \\\n",
    "        .setEvaluator(evaluator) \\\n",
    "        .setEstimatorParamMaps(paramGrid_gbm) \\\n",
    "        .setNumFolds(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_gbm_model = cv_gbm.fit(trainingData)\n",
    "cv_gbm_results = cv_gbm_model.transform(validationData)\n",
    "\n",
    "# Before we had 0.675 and now\n",
    "print(\"AUC: \" + str(evaluator.evaluate(cv_gbm_results)))\n",
    "\n",
    "# Means of model accuracy\n",
    "print(\"Means of metrics: \" + str(cv_gbm_model.avgMetrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply your best model to send the predictions on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
